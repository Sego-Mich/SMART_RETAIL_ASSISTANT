{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa6878b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Helper: Text preprocessing\n",
    "def preprocess_text(text, abbr_dict, pattern=None):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove pattern (e.g., URLs)\n",
    "    if pattern:\n",
    "        text = re.sub(pattern, '', text)\n",
    "\n",
    "    # 3. Expand abbreviations\n",
    "    words = text.split()\n",
    "    words = [abbr_dict.get(word, word) for word in words]\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    # 4. Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # 5. Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 6. Remove stopwords\n",
    "    stop_words = set(nltk_stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "    # 7. Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Helper: Clean text (fallback)\n",
    "def clean_text(text):\n",
    "    stop_words = ENGLISH_STOP_WORDS\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return [word for word in words if word not in stop_words]\n",
    "\n",
    "# Main function\n",
    "def find_suppliers(text, df):\n",
    "    print(\"Finding Suppliers...\")\n",
    "\n",
    "    try:\n",
    "        print(\"Using Model...\")\n",
    "\n",
    "        # Load abbreviation dictionary\n",
    "        with open('abbr.txt', 'r') as f:\n",
    "            abbr_dict = json.load(f)\n",
    "\n",
    "        # Load fitted vectorizer\n",
    "        with open(\"vectorizer.pkl\", \"rb\") as f:\n",
    "            vectorizer = pickle.load(f)\n",
    "\n",
    "        # Load fitted label encoder\n",
    "        with open(\"label_encoder.pkl\", \"rb\") as f:\n",
    "            label_encoder = pickle.load(f)\n",
    "\n",
    "        # Preprocess input\n",
    "        cleaned_text = preprocess_text(text, abbr_dict)\n",
    "        user_vec = vectorizer.transform([cleaned_text])  # Ensure match with training vectorizer\n",
    "\n",
    "         \n",
    "        best_model = load_model(\"model.keras\")\n",
    "        predicted_prob = best_model.predict(user_vec.toarray())\n",
    "        predicted_class = predicted_prob.argmax(axis=1)\n",
    "        predicted_label = label_encoder.inverse_transform(predicted_class)[0]\n",
    "\n",
    "        print(\"Predicted Label:\", predicted_label)\n",
    "\n",
    "        # Match in DataFrame\n",
    "        matches = df[df[\"Product Name\"] == predicted_label]\n",
    "        return matches[[\"Supplier/Vendor\", \"Unit Price\"]].iloc[:5].drop_duplicates()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Model failed. Falling back to dataset search. Reason: {e}\")\n",
    "        user_words = clean_text(text)\n",
    "\n",
    "        def match_row(desc):\n",
    "            desc = str(desc).lower()\n",
    "            return any(word in desc for word in user_words)\n",
    "\n",
    "        matches = df[df[\"Item Description\"].apply(match_row)]\n",
    "        return matches[[\"Supplier/Vendor\", \"Unit Price\", \"Item Description\"]].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3cd7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a36fc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Suppliers...\n",
      "Using Model...\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "Predicted Label: ACCOMMODATION\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Supplier/Vendor</th>\n",
       "      <th>Unit Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>BVGT</td>\n",
       "      <td>170.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>BVGT</td>\n",
       "      <td>8536.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>BVGT</td>\n",
       "      <td>426.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>BVGT</td>\n",
       "      <td>485.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>BVGT</td>\n",
       "      <td>194.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Supplier/Vendor  Unit Price\n",
       "73            BVGT      170.73\n",
       "74            BVGT     8536.59\n",
       "75            BVGT      426.83\n",
       "76            BVGT      485.77\n",
       "77            BVGT      194.31"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_suppliers('vendor accomodation', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e0040da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"best_model_overall.keras\"):\n",
    "            raise FileNotFoundError(\"Model file best_model_overall.keras is missing!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2988c31d",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8776/4162712668.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"best_model_overall.keras\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File contains:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1257\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1258\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'x'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m                 \u001b[1;31m# set the modified flag so central directory gets written\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1322\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File is not a zip file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1324\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File is not a zip file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"best_model_overall.keras\", 'r') as zip_ref:\n",
    "    print(\"File contains:\", zip_ref.namelist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e678a925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ntpath' from 'C:\\\\Users\\\\Mich\\\\Anaconda3\\\\lib\\\\ntpath.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a08143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
